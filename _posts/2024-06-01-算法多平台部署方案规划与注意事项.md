---
layout: mypost
title: 算法多平台部署方案规划与注意事项
categories: [文章]
published: true
date: 2024-06-01
tags: [文章]
---

# 前言
随着机器学习算法的成熟和算力储备的发展，越来越多的AI功能开始向边缘转移。我们最近也部署了很多的边缘计算方案，在设计端节点的边缘计算时，有如下几点重要的考量，在此记录，也分享给有兴趣的从业者。

## 框架选择
常见的框架有
- onnxruntime，微软旗下。适配性强，多平台易用性高
- tflite，谷歌旗下。早期的事实工业标准，目前迭代进度远迟于前沿需求，一些功能需要魔改或手动制作。
- CoreML，苹果专用框架。跨平台的服务商因成本考量实际不常使用。
- mnn，阿里旗下。C++框架，速度和能耗还不错。
- tnn，腾讯旗下，2023年后不不再维护。
- ncnn，腾讯旗下，C++为主，近期更新较为频繁。
- paddle lite，百度旗下，早期加入赛道较为成熟的框架。
- mxnet，Apache基金会旗下，火了一段时间后市场曝光量在逐步减少。

选择时考虑的因素从主到次分别为：
1. 性能
1. 能耗，以及能耗可控性
1. 多平台适配
1. 产物体积
1. 加密能力
1. 生产便利性
1. 新模块更新速度
1. 可扩展性

## 注意事项
1. 生产便利性虽然排在中间层次，但对于新项目开发者非常重要。onnx在胖端训练，瘦端服务，体验一致性较好，但coreml则需要使用其专用流程 create ml app来生产，虽然提供了转换工具，但对于定制的计算图不是很友好。
2. 降频测试，一定要在最糟糕的边界工况下测试推演能力，高频维持、降频前存续、降频后特性都必须经过严密测试，才能保证模型在服务环境达到可用效果。
3. 加密能力，对于希望保护知识产权的开发者，建议使用开源时间久，工业化较好的框架比如onnx、tflite，其中自带了对称、非对称加密的各种预设功能。对于必须使用某些不具备加密能力框架的情况，可以自行做对称加密，c动态库内存解密，基本能防范大多数的解包+java反编译，在有限的开发时间内达成符合性价比的安全性。

## 其他事项
1. 部分库在特定机型下生产、部署会遇到一些版本相关的问题，比如在`win11-23h2`+`python3.12`的研发环境中，`onnx 1.16.1` 可以适配运行，但其他版本经常有各种模糊报告的错误，影响了开发判断或拖慢了开发进度。
1. 类似的事情还可能出现在使用torch部署dx12胖算力时，`torch-directml` 的 `0.2.3.dev240715` 版本在AMD显卡+UHD显卡上运行正常，但是其他版本会对device报错，常见错误为`utf-8 cannot decode xxx`，导致无法创建张量，或无法训练。